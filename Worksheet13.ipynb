{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet: Neural Network\n",
    "\n",
    "One of the important questions in practice is tuning the hyperparameters such as the activation function, optimizers, batch size, epochs and etc. A lot of theories were established to select hyperparameters. However, many assumptions were made, which are hard to check in practice. \n",
    "\n",
    "In practice, we construct different models using different hyperparameters, and then compare the test performance of each model.\n",
    "\n",
    "In this worksheet, you will select the optimizer used to train a neural network. You will try **Adam optimizers**, **SGD optimizers**, **Adagrad optimizers**. Details about each optimizer are given below. For all other hyperparameters, they are selected as \n",
    "\n",
    "- Neural network structure: Shallow neural network with 200 neurons in the hidden layer. Activation function is ReLU. \n",
    "\n",
    "- Loss: This is regression problem so you should use MSE as loss and metric.\n",
    "\n",
    "\n",
    "- Number of epochs = 10, batch_size = 16\n",
    "\n",
    "\n",
    "**You should write two functions in this worksheet.** The first function takes training samples and minimizers as inputs, and returns a trained model. The second function takes test samples and trained model as inputs, and returns test error. Each function should work for any training samples, test samples, minimizers, and models.\n",
    "\n",
    "Train three models with different optimizers mentioned above using two functions you write, then report test error for each model.\n",
    "\n",
    "**Remark:** You may need to change default parameters in each optimizer to make the training process work, especially the SGD minimizer. \n",
    "\n",
    "**Grading policy:**\n",
    "1. Docstrings and in-line comments are added to explain your functions and codes.\n",
    "2. All optimizers work properly and 3 test errors are reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers:\n",
    "\n",
    "#### Adam\n",
    "\n",
    "Official paper: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.Adam(\n",
    "        \n",
    "        learning_rate=0.001, \n",
    "        beta_1=0.9, \n",
    "        beta_2=0.999, \n",
    "        epsilon=1e-07, \n",
    "        amsgrad=False,\n",
    "        name='Adam', \n",
    "        **kwargs)\n",
    "        \n",
    "Parameters:\n",
    "\n",
    "learning_rate: rate at which algorithm updates the parameter.  \n",
    "               Tensor or float type of value.Default value is 0.001\n",
    "\n",
    "beta_1: Exponential decay rate for 1st moment. Constant Float \n",
    "        tensor or float type of value. Default value is 0.9\n",
    "        \n",
    "beta_2: Exponential decay rate for 2nd moment. Constant Float \n",
    "        tensor or float type of value. Default value is 0.999\n",
    "        \n",
    "epsilon: Small value used to sustain numerical stability. \n",
    "         Floating point type of value. Default value is 1e-07\n",
    "         \n",
    "amsgrad: Whether to use AMSGrad variant or not. \n",
    "         Default value is False.\n",
    "         \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length\n",
    "\n",
    "#### RMSprop\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.RMSprop(\n",
    "    \n",
    "        learning_rate=0.001, \n",
    "        rho=0.9, \n",
    "        momentum=0.0, \n",
    "        epsilon=1e-07, \n",
    "        centered=False,\n",
    "        name='RMSprop', \n",
    "        **kwargs)\n",
    "\n",
    "Parameters:\n",
    "learning_rate: rate at which algorithm updates the parameter. \n",
    "               Tensor or float type of value.Default value is 0.001\n",
    "               \n",
    "rho: Discounting factor for gradients. Default value is 0.9\n",
    "\n",
    "momentum: accelerates rmsprop in appropriate direction. \n",
    "          Float type of value. Default value is 0.0\n",
    "          \n",
    "epsilon: Small value used to sustain numerical stability. \n",
    "         Floating point type of value. Default value is 1e-07\n",
    "         \n",
    "centered: By this gradients are normalised by the variance of \n",
    "          gradient. Boolean type of value. Setting value to True may\n",
    "          help with training model however it is computationally \n",
    "          more expensive. Default value if False.\n",
    "          \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Adagrad\n",
    "Syntax: \n",
    "\n",
    "    tf.keras.optimizers.Adagrad(\n",
    "    \n",
    "        learning_rate=0.001,\n",
    "        initial_accumulator_value=0.1,\n",
    "        epsilon=1e-07,\n",
    "        name=\"Adagrad\",\n",
    "        **kwargs)\n",
    "        \n",
    "Parameters: \n",
    "\n",
    "learning_rate: rate at which algorithm updates the parameter. \n",
    "               Tensor or float type of value.Default value is 0.001\n",
    "               \n",
    "initial_accumulator_value: Starting value for the per parameter \n",
    "                           momentum. Floating point type of value.\n",
    "                           Must be non-negative.Default value is 0.1\n",
    "                           \n",
    "epsilon: Small value used to sustain numerical stability. \n",
    "         Floating point type of value. Default value is 1e-07.\n",
    "         \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length\n",
    "\n",
    "#### SGD and its variations\n",
    "Syntax: \n",
    "\n",
    "    tf.kears.optimizers.SGD(\n",
    "        \n",
    "        learning_rate = 0.01,\n",
    "        momentum=0.0, \n",
    "        nesterov=False, \n",
    "        name='SGD', \n",
    "        **kwargs)\n",
    "        \n",
    "Parameters: \n",
    "\n",
    "learning_rate: rate at which algorithm updates the parameter. \n",
    "               Tensor or float type of value.Default value is 0.01\n",
    "               \n",
    "momentum: accelerates gradient descent in appropriate\n",
    "          direction. Float type of value. Default value is 0.0\n",
    "          \n",
    "nesterov: Whether or not to apply Nesterov Momentum.\n",
    "          Boolean type of value. Default value is False.\n",
    "          \n",
    "name: Optional name for the operation\n",
    "\n",
    "**kwargs: Keyworded variable length argument length.\n",
    "\n",
    "\n",
    "#### Some notes on different optimizers:\n",
    "1. https://www.geeksforgeeks.org/adam-optimizer/\n",
    "2. https://medium.com/aiÂ³-theory-practice-business/optimization-in-deep-learning-5a5d263172e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which optimizer should you use?\n",
    "\n",
    "- Convergence speed:\n",
    "\n",
    "SGD < SGD(momentum) = SGD(momentum, nesterov=TRUE) < Adagrad = RMSprop = Adam\n",
    "\n",
    "- Convergence quality:\n",
    "\n",
    "SGD = SGD(momentum) = SGD(momentum, nesterov=TRUE) >= RMSprop = Adam > Adagrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please run the following cell to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Luca\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Set to '1' to show warnings, '2' to show errors only, '3' to suppress all logs\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# generate data points\n",
    "datadim = 5              # intrinsic dimension of data\n",
    "dim = 500                # dimension of the ambient space\n",
    "N = 5000                 # data set size\n",
    "eps = 0.25               # noise level in the observaed data\n",
    "\n",
    "# create low dimensional data points\n",
    "Xreal = np.random.randn(N, datadim)\n",
    "y = np.tanh(Xreal[:,0]) + np.cos(Xreal[:,1]) - np.exp(-Xreal[:,4])\n",
    "\n",
    "# a matrix to embed the given data into a high-dimensional ambient space\n",
    "transform = np.random.randn(datadim,dim)\n",
    "\n",
    "# the high-dimensional data is obtained by matrix multiplication\n",
    "X = Xreal @ transform\n",
    "\n",
    "# making the observations noisy\n",
    "X += np.random.normal(0, eps, size = X.shape)\n",
    "y += np.random.normal(0, eps, size = y.shape)\n",
    "\n",
    "# Test data points:\n",
    "Ntest = 500\n",
    "Xrealtest = np.random.randn(Ntest, datadim)\n",
    "ytest = np.tanh(Xrealtest[:,0]) + np.cos(Xrealtest[:,1]) - np.exp(-Xrealtest[:,4])\n",
    "Xtest = Xrealtest@transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 4.0608 - mse: 4.0608\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 3.0107 - mse: 3.0107\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.9135 - mse: 1.9135\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8745 - mse: 1.8745\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3976 - mse: 1.3976\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4462 - mse: 1.4462\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2911 - mse: 1.2911\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1381 - mse: 1.1381\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1644 - mse: 1.1644\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0754 - mse: 1.0754\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 1.9004 - mse: 1.9004\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3481 - mse: 1.3481\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2905 - mse: 1.2905\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2750 - mse: 1.2750\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2525 - mse: 1.2525\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2224 - mse: 1.2224\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2133 - mse: 1.2133\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2414 - mse: 1.2414\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1934 - mse: 1.1934\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1872 - mse: 1.1872\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5538 - mse: 1.5538\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2499 - mse: 1.2499\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1813 - mse: 1.1813\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1665 - mse: 1.1665\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1216 - mse: 1.1216\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1173 - mse: 1.1173\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0893 - mse: 1.0893\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0634 - mse: 1.0634\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0640 - mse: 1.0640\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.0549 - mse: 1.0549\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 5.5650 - mse: 5.5650\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 2.8924 - mse: 2.8924\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8889 - mse: 1.8889\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.4265 - mse: 1.4265\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3327 - mse: 1.3327\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2711 - mse: 1.2711\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2582 - mse: 1.2582\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1946 - mse: 1.1946\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.1834 - mse: 1.1834\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 1.2099 - mse: 1.2099\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Test error with Adam optimizer: 0.3671846143904421\n",
      "Test error with SGD optimizer: 0.42106051200398154\n",
      "Test error with Adagrad optimizer: 0.4612084479734086\n",
      "Test error with RMSprop optimizer: 0.3936385244511009\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define the model training function\n",
    "def train_model(X_train, y_train, optimizer):\n",
    "    \"\"\"\n",
    "    Trains a neural network model on the provided training data using the specified optimizer.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (ndarray): Training input data\n",
    "    y_train (ndarray): Training target data\n",
    "    optimizer (tf.keras.optimizers.Optimizer): Optimizer to be used for training\n",
    "    \n",
    "    Returns:\n",
    "    model (tf.keras.Model): Trained neural network model\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(200, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=1)\n",
    "    return model\n",
    "\n",
    "# Define the model evaluation function\n",
    "def evaluate_model(X_test, y_test, model):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on test data and returns the test error.\n",
    "    \n",
    "    Parameters:\n",
    "    X_test (ndarray): Test input data\n",
    "    y_test (ndarray): Test target data\n",
    "    model (tf.keras.Model): Trained neural network model\n",
    "    \n",
    "    Returns:\n",
    "    test_error (float): Mean squared error on the test data\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Ensure there are no NaNs in predictions before calculating MSE\n",
    "    if np.isnan(y_pred).any():\n",
    "        raise ValueError(\"Prediction contains NaN values. Check optimizer parameters.\")\n",
    "    \n",
    "    test_error = mean_squared_error(y_test, y_pred)\n",
    "    return test_error\n",
    "\n",
    "# Initialize optimizers\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True, clipnorm=1.0)\n",
    "adagrad_optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)\n",
    "rmsprop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "\n",
    "# Train models with different optimizers\n",
    "adam_model = train_model(X, y, adam_optimizer)\n",
    "sgd_model = train_model(X, y, sgd_optimizer)\n",
    "adagrad_model = train_model(X, y, adagrad_optimizer)\n",
    "rmsprop_model = train_model(X, y, rmsprop_optimizer)\n",
    "\n",
    "# Evaluate each model and print test errors\n",
    "adam_test_error = evaluate_model(Xtest, ytest, adam_model)\n",
    "sgd_test_error = evaluate_model(Xtest, ytest, sgd_model)\n",
    "adagrad_test_error = evaluate_model(Xtest, ytest, adagrad_model)\n",
    "rmsprop_test_error = evaluate_model(Xtest, ytest, rmsprop_model)\n",
    "\n",
    "print(\"Test error with Adam optimizer:\", adam_test_error)\n",
    "print(\"Test error with SGD optimizer:\", sgd_test_error)\n",
    "print(\"Test error with Adagrad optimizer:\", adagrad_test_error)\n",
    "print(\"Test error with RMSprop optimizer:\", rmsprop_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
